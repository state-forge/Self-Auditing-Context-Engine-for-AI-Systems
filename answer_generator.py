import requests
import json
def ans_gen(query, results):

    #STEP1:Extract Retrieved Context
    retrieved_chunks=[]

    if not results:
        raise ValueError("No documents retrieved for the query")

    for idx, doc in enumerate(results):
        retrieved_chunks.append({
            "chunk_id":f"C{idx+1}",
            "text":doc.page_content.strip()
        })
    
    #STEP2:Build Prompt
    def build_prompt(query, chunks):
        context_block=""
        for chunk in chunks:
            context_block+=f"[{chunk['chunk_id']}]\n{chunk['text']}\n\n"
        
        prompt=f"""
            You are an assistant answering a question using ONLY the provided text snippets.

            Question:
            {query}

            Retrieved Text Chunks:
            {context_block}

            Instructions:
            1. Synthesize the answer by combining information across all retrieved text snippets.
            2. Before writing the paragraph, identify all major themes present in the retrieved text (e.g., causes, impacts, regions, time scales), and ensure the paragraph covers as many of these themes as possible.
            3. Write a single, well-structured paragraph of upto 300 words(if sufficient information is available) answering the question. The paragraph must be fully based on the retrieved text only.
            4. Write a single coherent paragraph. The answer should be upto 7-10 sentences if sufficient information is available.
            5. Do not include external knowledge.
            6. If the answer is not present, say "The retrieved text does not contain sufficient information."
            7. Output STRICT JSON only. Do NOT include explanations or extra text.

            Output Format (STRICT JSON):
            {{
            "answer": "...",
            "supporting_chunks": ["C1", "C2"]
            }}
            """
        return prompt
    
    #STEP3:Call LLaMA-3 via Ollama

    OLLAMA_URL="http://localhost:11434/api/generate"

    prompt = build_prompt(query, retrieved_chunks)

    payload={
        "model":"llama3:8b",
        "prompt":prompt,
        "stream":False,
        "options":{
            "temperature":0.0,
            "num_predict":1000
        }
    }

    response=requests.post(OLLAMA_URL,json=payload, timeout=120)

    if response.status_code!=200:
        raise RuntimeError("Ollama request failed")
    
    response_json=response.json()
    if "response" not in response_json:
        raise RuntimeError("Ollama response missing 'response' field")
    raw_output=response_json["response"]

    #STEP4:Parse & Validate Output
    try:
        parsed_output=json.loads(raw_output)
    except json.JSONDecodeError:
        raise ValueError("LLM output is not a valid JSON")
    
    answer=parsed_output.get("answer", "No answer generated.").strip()
    supporting_chunks=parsed_output.get("supporting_chunks",[])

    if not answer:
        raise ValueError("Empty answer generated by LLM")

    if not isinstance(supporting_chunks, list):
        raise ValueError("supporting_chunks must be a list")
    
    valid_ids={c["chunk_id"] for c in retrieved_chunks}

    # if answer and not any(cid in valid_ids for cid in supporting_chunks):
    #     raise ValueError("Answer generated without valid supporting evidence")

    print("\nFinal Answer : \n")
    print(answer)
    return